# MultiHashTopK

## 问题描述

 有100GB的URL文件，限制1GB内存，求Top100的URL以及出现次数 

## 问题分析

- 海量数据计算频率Topk问题，采用 hash切分文件 + hashmap + 最小堆维护Topk 的思路。理想情况下，hash将100G文件切成100份1G文件；每次读取1GB文件到内存中，用hashmap统计url的频率；维护一个TopK的最小堆，由于k=100比较小，可以将这个堆一直放在内存中用来比较；读取100次文件后，最后堆里的url即为TopK。
- 采取hash切分文件，而不是随意将文件分割成100份1G的原因就在于，不希望同一个url出现在多个文件中。否则，统计url的频数需要对所有文件进行归并统计，IO次数多。
- 用hash分散url的时候，hash bucket越多，冲突越少，数据分布更趋向均匀。但是，为了减少IO次数，我们要限定hash切分的文件数目。这是一个trade-off。

## 解决方案

- 采用Multi-hash的方式。针对任意两个字符串s1, s2，由于`P(h1(s1)==h1(s2)&&h2(s1)==h2(s2)) < P(h1(s1)==h1(s2)) `，其中p为概率，我们可以用一个字符串hash函数 h1 将原来的100G切成100份。选取其中大小超过1G的文件，用另一个hash函数h2进行划分。因为我们希望数据尽量均匀，所以如果该文件大小为2.5G，我们可以将h2的hash bucket设为3。如果h2划分的文件还有超过1G的，继续用h3划分，之后h4, h5等等。因为每次读取超标size的文件进行划分也是IO，所以我们可以将hash过程限制为4轮。4轮之后，如果还有size超过1G的文件，我们认为它里面全是同一个url的重复，该url的频数可以直接计算，并且直接成为Top100中的一个。
- 将最高效的字符串hash函数放在第一轮，尽量减少冲突，减少超标size的文件数目，减少后续的读取和re-hash写入。
- 每一轮的超标size文件被rehash之后，本身会被删除。

## 代码

- 100G文件，1G内存问题缩小为 1G文件，10M内存。

- `src/original`中有从各个视频网站爬来的url集合，`src/GenerateURL` 将这些url进行多次重复写入到`URL.txt`中构成原始文件。选取域名前缀为`www.bilibili.com`的100个url进行最多重复作为Top100.
- `src/HashSplit` 对原始文件进行单次hash切分。将bucket设为100时，超标size的文件很多。设置为200，依然有一些。
- `src/MultiHashSplit` 对原始文件进行四轮hash切分，选取的四个高效字符串hash函数分别为 BKDRHash, APHash, DJBHash, JSHash。一共会产生200+临时文件，但是最后剩下的文件一般不超过160个。且每个文件大小不超过10M。（基本第3，4轮hash都用不上）
- `src/TopK` 遍历所有的文件，得到`Top100.txt`. （Top100中偶尔会出现一个不是bilibili的url，而是不小心从多个网站爬出来的同一个url,  http://12377.cn/ ，可以点进去自行欣赏。)
- `test/HashSplitTest` 和 `test/MultiHashSplitTest` 都是对 hash的 “不丢失任何一个url” 和 “同一个url不会出现在多个文件中” 特性进行测试。
- `test/TopKTest` 对 hashmap+最小堆TopK 在单个文件和多个文件进行正确性测试。

## 不足之处

- 其实多重hash最常用的是双重hash，其中两个hash满足一定的数学关系，尽量避免冲突。但是为了减少复杂度，本次直接上了四轮hash去判断。
- 上一轮hash产生的超标size临时文件，下一轮一定会被读。最坏情况下，每个临时文件都会被写和读一次，也就是说如果一共产生250个临时文件，最坏情况下，进行了500次IO。（优点是，最后剩下的文件数目一般会很小，方便后面的计算）

